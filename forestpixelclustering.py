# -*- coding: utf-8 -*-
"""ForestPixelClustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Ovg8uwhEPTW-CYdoXNiz2gpmRZQ8Yif

### This notebook performs Clustering on the forest pixels.

#### Part 1: Preprocessing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("pixel_spectral_data.csv")

##
print("Checking the data")
assert np.all(data.dtypes==np.float64) , "Unknown datatype, Please check"
assert np.all(data.isnull().sum()==0),"Dataframe has null values, Please check"
print(f'shape of df: {data.shape}')

##
print(data.skew())
skewness = data.skew()
skewness = skewness.dropna()

skewness.plot(kind='bar', figsize=(10, 6))
plt.xlabel('Column')
plt.ylabel('Skewness')
plt.title('Skewness of Columns')
plt.show()

##

##
col = 'NDVI'
sns.histplot(data=df, x=col, kde=True, color="skyblue")

#Outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.xlabel('Column')
plt.ylabel('Value')
plt.title('Outliers in Columns')
plt.show()
###
z_scores = np.abs(stats.zscore(data1))
outlier_indexes = np.where(z_scores > z_score_threshold)
datanew = df.drop(outlier_indexes[0]).reset_index(drop=True)
perc = (datdfa.shape[0] - datanew.shape[0])*100/(data.shape[0])
print(f"Number of outliers detected: {df.shape[0]-datanew.shape[0]} ( {perc:.2f}%)")



"""#### Part 2 : Finding optimal clustering mechanism and number of clusters

##### Part 2.1 K means
"""

from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.cluster import AffinityPropagation
from sklearn import metrics
from sklearn.mixture import GaussianMixture

km = KMeans(n_clusters = 5,init = 'k-means++',n_init =10)
labels = km.fit_predict(df)
print(labels)

min = 2
max = 10
for k in range(min,max+1):
    km = KMeans(n_clusters = k)
    km.fit(data)
    "1. Kmeans error"
    sse.append(km.inertia_)
    "2. Silhouette score"
    score = silhouette_score(data,km.labels_)
    sil_scores.append(score)
    "3. Silhouette visualization"
    visualizer = SilhouetteVisualizer(km)
    visualizer.fit(data)
    visualizer.show()
plt.figure(1)
sns.set_style("whitegrid")
g=sns.lineplot(x=range(min,max+1), y=sse,ax = axs)
g.set(xlabel ="Number of cluster (k)",
  ylabel = "Sum Squared Error",
  title ='Elbow Method')
plt.figure(2)
plt.plot(range(min,max+1), sil_scores)
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Each K')
plt.show()

X = df
    def gmm_bic_score(estimator, X):
      """Callable to pass to GridSearchCV that will use the BIC score."""
      # Make it negative since GridSearchCV expects a score to maximize
      return -estimator.bic(X)

    param_grid = {  "n_components": range(1, 20),
                 "covariance_type": ["spherical", "tied", "diag", "full"],
                 }
    cov_types = ["spherical", "tied", "diag", "full"]
    bics = []
    models=[]
    for ncomp in range(1,20):
          gmm = GaussianMixture(n_components = ncomp,covariance_type= cov_type)
          gmm.fit(X)
          bics.append(gmm.bic(X))
          models.append(gmm)
    plt.plot(range(len(bics)),bics)
    return
    grid_search = GridSearchCV(
        GaussianMixture(), param_grid=param_grid, scoring=gmm_bic_score)
    grid_search.fit(X)

    "Plot"
    df = pd.DataFrame(grid_search.cv_results_)[["param_n_components", "param_covariance_type", "mean_test_score"]]
    df["mean_test_score"] = -df["mean_test_score"]
    df = df.rename(
        columns={
        "param_n_components": "Number of components",
        "param_covariance_type": "Type of covariance",
        "mean_test_score": "BIC score",})
    df.sort_values(by="BIC score").head()
    sns.catplot( data=df,kind="bar", x="Number of components", y="BIC score", hue="Type of covariance",)
    plt.show()